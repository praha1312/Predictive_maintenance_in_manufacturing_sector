{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7114acb-83ed-4a3b-8621-4901c30f497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88568b96-3319-4980-b5ad-c1daa40bbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577c339-5429-4daa-9194-067fe54b77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_and_train_model(data_path):\n",
    "    \"\"\"\n",
    "    Prepare data and train model for maintenance prediction\n",
    "    \"\"\"\n",
    "    # Read the data\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Convert timestamp to datetime with the correct format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # 1. Extract time-based features from timestamp\n",
    "    df['day_of_month'] = df['timestamp'].dt.day\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    \n",
    "    # 2. Create rolling window features for each sensor (last 7 days)\n",
    "    sensors = ['temperature', 'vibration', 'noise_level']\n",
    "    for sensor in sensors:\n",
    "        # Rolling mean\n",
    "        df[f'{sensor}_7day_mean'] = df.groupby('machine_id')[sensor].rolling(\n",
    "            window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        \n",
    "        # Rolling std\n",
    "        df[f'{sensor}_7day_std'] = df.groupby('machine_id')[sensor].rolling(\n",
    "            window=7, min_periods=1).std().reset_index(0, drop=True)\n",
    "        \n",
    "        # Rate of change (difference from previous day)\n",
    "        df[f'{sensor}_change'] = df.groupby('machine_id')[sensor].diff()\n",
    "        \n",
    "        # Distance from normal operating range\n",
    "        if sensor == 'temperature':\n",
    "            normal_range = (60, 70)  # for CNC machines\n",
    "        elif sensor == 'vibration':\n",
    "            normal_range = (0.5, 0.9)\n",
    "        else:  # noise_level\n",
    "            normal_range = (65, 75)\n",
    "            \n",
    "        df[f'{sensor}_deviation'] = df[sensor].apply(\n",
    "            lambda x: max(0, x - normal_range[1]) + min(0, x - normal_range[0])\n",
    "        )\n",
    "    \n",
    "    # 3. Calculate running hours statistics\n",
    "    df['hours_run_7day_mean'] = df.groupby('machine_id')['hours_run'].rolling(\n",
    "        window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['hours_run_7day_std'] = df.groupby('machine_id')['hours_run'].rolling(\n",
    "        window=7, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # 4. Encode machine type\n",
    "    le = LabelEncoder()\n",
    "    df['machine_type_encoded'] = le.fit_transform(df['machine_type'])\n",
    "    \n",
    "    # Define features for the model\n",
    "    feature_columns = [\n",
    "        'temperature', 'vibration', 'noise_level', 'hours_run',\n",
    "        'machine_type_encoded', 'day_of_month', 'month',\n",
    "        'temperature_7day_mean', 'temperature_7day_std', 'temperature_change', 'temperature_deviation',\n",
    "        'vibration_7day_mean', 'vibration_7day_std', 'vibration_change', 'vibration_deviation',\n",
    "        'noise_level_7day_mean', 'noise_level_7day_std', 'noise_level_change', 'noise_level_deviation',\n",
    "        'hours_run_7day_mean', 'hours_run_7day_std'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=feature_columns + ['machine_health'])\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[feature_columns]\n",
    "    y = df['machine_health']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create and train XGBoost model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        callbacks=[xgb.callback.EarlyStopping(rounds=10)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train,\n",
    "        eval_set=[(X_test_scaled, y_test)],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(importance_df['feature'][:10], importance_df['importance'][:10])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Top 10 Most Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nActual vs Predicted Health Scores (Sample):\")\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Actual': y_test.iloc[:5],\n",
    "        'Predicted': y_pred[:5],\n",
    "        'Difference': abs(y_test.iloc[:5] - y_pred[:5])\n",
    "    })\n",
    "    print(comparison_df)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': le,\n",
    "        'feature_columns': feature_columns,\n",
    "        'metrics': {\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        },\n",
    "        'feature_importance': importance_df\n",
    "    }\n",
    "\n",
    "def predict_health(model_artifacts, new_data):\n",
    "    \"\"\"\n",
    "    Predict machine health for new data\n",
    "    \"\"\"\n",
    "    df_pred = new_data.copy()\n",
    "    \n",
    "    # Apply the same feature engineering\n",
    "    df_pred['timestamp'] = pd.to_datetime(df_pred['timestamp'])\n",
    "    df_pred['day_of_month'] = df_pred['timestamp'].dt.day\n",
    "    df_pred['month'] = df_pred['timestamp'].dt.month\n",
    "    \n",
    "    # Calculate rolling features\n",
    "    sensors = ['temperature', 'vibration', 'noise_level']\n",
    "    for sensor in sensors:\n",
    "        df_pred[f'{sensor}_7day_mean'] = df_pred.groupby('machine_id')[sensor].rolling(\n",
    "            window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df_pred[f'{sensor}_7day_std'] = df_pred.groupby('machine_id')[sensor].rolling(\n",
    "            window=7, min_periods=1).std().reset_index(0, drop=True)\n",
    "        df_pred[f'{sensor}_change'] = df_pred.groupby('machine_id')[sensor].diff()\n",
    "        \n",
    "        if sensor == 'temperature':\n",
    "            normal_range = (60, 70)\n",
    "        elif sensor == 'vibration':\n",
    "            normal_range = (0.5, 0.9)\n",
    "        else:  # noise_level\n",
    "            normal_range = (65, 75)\n",
    "            \n",
    "        df_pred[f'{sensor}_deviation'] = df_pred[sensor].apply(\n",
    "            lambda x: max(0, x - normal_range[1]) + min(0, x - normal_range[0])\n",
    "        )\n",
    "    \n",
    "    df_pred['hours_run_7day_mean'] = df_pred.groupby('machine_id')['hours_run'].rolling(\n",
    "        window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df_pred['hours_run_7day_std'] = df_pred.groupby('machine_id')['hours_run'].rolling(\n",
    "        window=7, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Encode machine type\n",
    "    df_pred['machine_type_encoded'] = model_artifacts['label_encoder'].transform(df_pred['machine_type'])\n",
    "    \n",
    "    # Select and scale features\n",
    "    X_pred = df_pred[model_artifacts['feature_columns']]\n",
    "    X_pred_scaled = model_artifacts['scaler'].transform(X_pred)\n",
    "    \n",
    "    # Make predictions\n",
    "    health_predictions = model_artifacts['model'].predict(X_pred_scaled)\n",
    "    \n",
    "    return health_predictions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_results = prepare_and_train_model('maintenance_data.csv')\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"RMSE: {model_results['metrics']['rmse']:.2f}\")\n",
    "    print(f\"RÂ² Score: {model_results['metrics']['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    print(model_results['feature_importance'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf844da-bc6c-431b-a9a8-1bf35d479b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'maintenance_data.csv'  # Replace with the actual path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Add a random 'Health_Status' column\n",
    "health_status_options = ['Healthy', 'Warning', 'Critical']\n",
    "\n",
    "# Generate random health status for each row in the dataset\n",
    "np.random.seed(42)\n",
    "data['Health_Status'] = np.random.choice(health_status_options, size=len(data))\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb03170-2117-4d04-bd9f-389b4951baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical columns to numeric values\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[column] = label_encoder.fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba62cd2-5e2d-4aea-bf63-9943017a570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a random 'Health_Status' column\n",
    "health_status_options = ['Healthy', 'Warning', 'Critical']\n",
    "np.random.seed(42)\n",
    "data['Health_Status'] = np.random.choice(health_status_options, size=len(data))\n",
    "\n",
    "# Convert categorical columns to numeric using LabelEncoder\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "\n",
    "target_column = 'Health_Status'\n",
    "features = data.drop(columns=[target_column])\n",
    "target = data[target_column]\n",
    "\n",
    "# Handle missing values \n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict health status for new data\n",
    "health_predictions = label_encoder.inverse_transform(model.predict(X_test))\n",
    "print(\"Predicted Health Status:\", health_predictions)\n",
    "\n",
    "# Handle unexpected non-numeric values\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check for NaN values after conversion and fill them with the mean\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "if 'cycle' in data.columns and 'failure' in data.columns:\n",
    "    data['RUL'] = data.groupby('unit')['cycle'].transform('max') - data['cycle']\n",
    "else:\n",
    "    data['cycle'] = np.random.randint(1, 100, size=len(data))\n",
    "    data['failure'] = np.random.choice([0, 1], size=len(data))\n",
    "    data['RUL'] = np.random.randint(1, 100, size=len(data))\n",
    "\n",
    "# Split the data into features and target (RUL)\n",
    "target_column = 'RUL'\n",
    "features = data.drop(columns=[target_column])\n",
    "target = data[target_column]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the XGBoost Regressor model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict RUL for new data\n",
    "rul_predictions = model.predict(X_test)\n",
    "print(\"Predicted RUL:\", rul_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f475ab1-82d7-479b-994b-2578f5cf0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the feature data\n",
    "anomaly_pred = iso_forest.fit_predict(features_scaled)\n",
    "\n",
    "# Convert predictions: -1 for anomaly, 1 for normal\n",
    "anomalies = np.where(anomaly_pred == -1, 1, 0)\n",
    "\n",
    "# Add anomaly labels to the data for inspection\n",
    "data['Anomaly'] = anomalies\n",
    "\n",
    "# Check the data with anomaly labels\n",
    "print(data[['Anomaly'] + list(features.columns)].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e957f-2471-4a11-aed9-9f9c41f3f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model to identify the important features\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, anomalies, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), importances[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), features.columns[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance for Anomaly Detection\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31f242-b42a-4f7a-803b-e10262055305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CNC machine components\n",
    "components_list = [\n",
    "    'Controller', 'Drives (Motors)', 'Spindle', 'Linear Guides', \n",
    "    'Ball Screws', 'Table', 'CNC Tool', 'Coolant System', \n",
    "    'Feedback Sensors', 'Power Supply', 'Operator Interface', 'Safety Systems'\n",
    "]\n",
    "\n",
    "# Add a column named 'components' with all components listed above\n",
    "data['components'] = ', '.join(components_list)\n",
    "\n",
    "print(data[['components']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223406fe-50dd-4871-a358-ccf3dca55f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isolation Forest to detect anomalies\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model to the features\n",
    "anomaly_pred = iso_forest.fit_predict(features_scaled)\n",
    "\n",
    "# Convert predictions: -1 for anomaly, 1 for normal\n",
    "anomalies = np.where(anomaly_pred == -1, 1, 0)\n",
    "\n",
    "# Add the anomaly labels to the dataset\n",
    "data['Anomaly'] = anomalies\n",
    "\n",
    "# Check the first few rows with anomaly labels\n",
    "print(data[['Anomaly', 'components']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d0c8c-c077-4f7a-8838-7164977911fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CNC machine components\n",
    "components_list = [\n",
    "    'Controller', 'Drives (Motors)', 'Spindle', 'Linear Guides', \n",
    "    'Ball Screws', 'Table', 'CNC Tool', 'Coolant System', \n",
    "    'Feedback Sensors', 'Power Supply', 'Operator Interface', 'Safety Systems'\n",
    "]\n",
    "\n",
    "# Function to identify components likely to fail based on anomaly detection and feature importance\n",
    "def get_failing_components_per_machine(anomaly_labels, feature_importances, components_list, importance_threshold=0.1):\n",
    "    failing_components = []\n",
    "    \n",
    "    # Determine which components have high importance (greater than the threshold)\n",
    "    for i, importance in enumerate(feature_importances):\n",
    "        if importance > importance_threshold:\n",
    "            failing_components.append(components_list[i])\n",
    "    \n",
    "    # If an anomaly is detected, return the list of likely failing components for this machine\n",
    "    if anomaly_labels == 1:\n",
    "        return ', '.join(failing_components)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to each row of the dataset\n",
    "data['components_about_to_fail'] = data.apply(\n",
    "    lambda row: get_failing_components_per_machine(row['Anomaly'], model.feature_importances_, components_list, importance_threshold=0.1),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(data[['components', 'Anomaly', 'components_about_to_fail']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ed98d-b10a-4ddd-bb28-81cccf7fa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Remaining Life\n",
    "data['Simulated_Remaining_Life'] = np.random.randint(1, 100, size=len(data))\n",
    "\n",
    "# Failure Risk Score (arbitrary, for demonstration purposes)\n",
    "data['Failure_Risk_Score'] = np.random.randint(1, 11, size=len(data))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert all categorical columns to numeric values\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Split the data into features and target\n",
    "target_column = 'Simulated_Remaining_Life'\n",
    "features = data.drop(columns=[target_column])\n",
    "target = data[target_column]\n",
    "\n",
    "# Handle missing values if any\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_scaled, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train an XGBoost Regressor to predict remaining life\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict Simulated Remaining Life and Failure Risk Score\n",
    "predicted_remaining_life = model.predict(X_test)\n",
    "\n",
    "# Add predictions back to the dataset for display\n",
    "output_df = pd.DataFrame({\n",
    "    'Simulated_Remaining_Life': y_test,  # True values\n",
    "    'Predicted_Simulated_Remaining_Life': predicted_remaining_life,\n",
    "    'Failure_Risk_Score': data.loc[y_test.index, 'Failure_Risk_Score'],  # True failure scores\n",
    "    'Predicted_Failure_Risk_Score': np.random.uniform(9, 11, len(y_test))  # Simulated predictions\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(output_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
